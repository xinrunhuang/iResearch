{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBw57RUmaQgf",
        "outputId": "f3bd4369-63a4-40ff-8015-1a23bea3ac85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuxYDeVkzY4P"
      },
      "source": [
        "# Setting environment and pre-load models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BreVU84n4HuF"
      },
      "source": [
        "## build pdf2json env ( java )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjzYtKgKzd-J",
        "outputId": "1ae6fa62-5562-4ba0-cca8-d8ca4fc66f79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   520  100   520    0     0  16774      0 --:--:-- --:--:-- --:--:-- 16774\n",
            "Update environment...\n",
            "Install Java...\n",
            "Install Jupyter java kernel...\n",
            "jdk.jshell@11.0.14.1\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://raw.githubusercontent.com/deepjavalibrary/d2l-java/master/tools/colab_build.sh && bash colab_build.sh\n",
        "!java --list-modules | grep \"jdk.jshell\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh-bIWVwzmBZ",
        "outputId": "abaf588c-2639-43fc-90ce-a2314213d5a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 's2orc-doc2json'...\n",
            "remote: Enumerating objects: 480, done.\u001b[K\n",
            "remote: Counting objects: 100% (279/279), done.\u001b[K\n",
            "remote: Compressing objects: 100% (187/187), done.\u001b[K\n",
            "remote: Total 480 (delta 166), reused 181 (delta 88), pack-reused 201\u001b[K\n",
            "Receiving objects: 100% (480/480), 9.40 MiB | 9.75 MiB/s, done.\n",
            "Resolving deltas: 100% (255/255), done.\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (4.63.0)\n",
            "Collecting beautifulsoup4==4.7.1\n",
            "  Downloading beautifulsoup4-4.7.1-py3-none-any.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting boto3==1.9.147\n",
            "  Downloading boto3-1.9.147-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 71.4 MB/s \n",
            "\u001b[?25hCollecting requests==2.21.0\n",
            "  Downloading requests-2.21.0-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting Flask==1.0.2\n",
            "  Downloading Flask-1.0.2-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 11.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (4.2.6)\n",
            "Collecting python-magic==0.4.18\n",
            "  Downloading python_magic-0.4.18-py2.py3-none-any.whl (8.6 kB)\n",
            "Collecting latex2mathml==2.16.2\n",
            "  Downloading latex2mathml-2.16.2-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.7/dist-packages (from beautifulsoup4==4.7.1->-r requirements.txt (line 2)) (2.3.2)\n",
            "Collecting s3transfer<0.3.0,>=0.2.0\n",
            "  Downloading s3transfer-0.2.1-py2.py3-none-any.whl (70 kB)\n",
            "\u001b[K     |████████████████████████████████| 70 kB 8.5 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting botocore<1.13.0,>=1.12.147\n",
            "  Downloading botocore-1.12.253-py2.py3-none-any.whl (5.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7 MB 68.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r requirements.txt (line 4)) (2021.10.8)\n",
            "Collecting idna<2.9,>=2.5\n",
            "  Downloading idna-2.8-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.7/dist-packages (from Flask==1.0.2->-r requirements.txt (line 5)) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug>=0.14 in /usr/local/lib/python3.7/dist-packages (from Flask==1.0.2->-r requirements.txt (line 5)) (1.0.1)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask==1.0.2->-r requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask==1.0.2->-r requirements.txt (line 5)) (1.1.0)\n",
            "Collecting docutils<0.16,>=0.10\n",
            "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
            "\u001b[K     |████████████████████████████████| 547 kB 68.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.13.0,>=1.12.147->boto3==1.9.147->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10->Flask==1.0.2->-r requirements.txt (line 5)) (2.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.13.0,>=1.12.147->boto3==1.9.147->-r requirements.txt (line 3)) (1.15.0)\n",
            "Installing collected packages: jmespath, docutils, botocore, s3transfer, idna, requests, python-magic, latex2mathml, Flask, boto3, beautifulsoup4\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.17.1\n",
            "    Uninstalling docutils-0.17.1:\n",
            "      Successfully uninstalled docutils-0.17.1\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: Flask\n",
            "    Found existing installation: Flask 1.1.4\n",
            "    Uninstalling Flask-1.1.4:\n",
            "      Successfully uninstalled Flask-1.1.4\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.21.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed Flask-1.0.2 beautifulsoup4-4.7.1 boto3-1.9.147 botocore-1.12.253 docutils-0.15.2 idna-2.8 jmespath-0.10.0 latex2mathml-2.16.2 python-magic-0.4.18 requests-2.21.0 s3transfer-0.2.1\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/allenai/s2orc-doc2json.git\n",
        "import os\n",
        "os.chdir('/content/s2orc-doc2json')\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "os.chdir('/content')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkjcvMF9Ztli"
      },
      "source": [
        "## install python library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssD-JWCJNUl_"
      },
      "source": [
        "重新启动应用程序\n",
        "restart the kernal after installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r2reqMlagBV",
        "outputId": "fea66400-7254-445c-b654-8d7f12d75227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Requirement already satisfied: telepot in /usr/local/lib/python3.7/dist-packages (12.7)\n",
            "Requirement already satisfied: urllib3>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from telepot) (1.26.9)\n",
            "Requirement already satisfied: aiohttp>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from telepot) (3.8.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.0.0->telepot) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.0.0->telepot) (1.7.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.0.0->telepot) (21.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.0.0->telepot) (3.10.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.0.0->telepot) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.0.0->telepot) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.0.0->telepot) (1.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.0.0->telepot) (6.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.0.0->telepot) (2.0.12)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp>=3.0.0->telepot) (2.8)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Requirement already satisfied: google-cloud-dialogflow in /usr/local/lib/python3.7/dist-packages (2.13.0)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.7/dist-packages (from google-cloud-dialogflow) (2.7.1)\n",
            "Requirement already satisfied: proto-plus>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-dialogflow) (1.20.3)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-dialogflow) (3.20.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-dialogflow) (2.27.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-dialogflow) (1.56.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-dialogflow) (1.35.0)\n",
            "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-dialogflow) (1.44.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-dialogflow) (1.44.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-dialogflow) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-dialogflow) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-dialogflow) (0.2.8)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-dialogflow) (57.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-dialogflow) (4.8)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-dialogflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-dialogflow) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-dialogflow) (1.26.9)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-dialogflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-dialogflow) (2.8)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Requirement already satisfied: scikit-learn==0.22.1 in /usr/local/lib/python3.7/dist-packages (0.22.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.1) (1.21.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.1) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.1) (1.1.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Requirement already satisfied: json2html in /usr/local/lib/python3.7/dist-packages (1.3.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Requirement already satisfied: pdfkit in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "--2022-04-13 10:46:42--  https://github.com/wkhtmltopdf/packaging/releases/download/0.12.6-1/wkhtmltox_0.12.6-1.bionic_amd64.deb\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/131323182/b6d71780-ab7e-11ea-9b13-e2875e48ec6c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220413%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220413T104622Z&X-Amz-Expires=300&X-Amz-Signature=6cc0958bf8407cd41800a4768c01d79a735dacd260a0acf83ac602d9c331f3de&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=131323182&response-content-disposition=attachment%3B%20filename%3Dwkhtmltox_0.12.6-1.bionic_amd64.deb&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-04-13 10:46:42--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/131323182/b6d71780-ab7e-11ea-9b13-e2875e48ec6c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220413%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220413T104622Z&X-Amz-Expires=300&X-Amz-Signature=6cc0958bf8407cd41800a4768c01d79a735dacd260a0acf83ac602d9c331f3de&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=131323182&response-content-disposition=attachment%3B%20filename%3Dwkhtmltox_0.12.6-1.bionic_amd64.deb&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15729530 (15M) [application/octet-stream]\n",
            "Saving to: ‘wkhtmltox_0.12.6-1.bionic_amd64.deb.2’\n",
            "\n",
            "wkhtmltox_0.12.6-1. 100%[===================>]  15.00M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-04-13 10:46:42 (253 MB/s) - ‘wkhtmltox_0.12.6-1.bionic_amd64.deb.2’ saved [15729530/15729530]\n",
            "\n",
            "E: dpkg was interrupted, you must manually run 'sudo dpkg --configure -a' to correct the problem. \n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Requirement already satisfied: mkwikidata in /usr/local/lib/python3.7/dist-packages (0.14)\n",
            "Requirement already satisfied: requests>=2.25.1 in /usr/local/lib/python3.7/dist-packages (from mkwikidata) (2.27.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->mkwikidata) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->mkwikidata) (2.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->mkwikidata) (1.26.9)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->mkwikidata) (2.0.12)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.7/dist-packages (0.0.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.5.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.0.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.2.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.15)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.63.0)\n",
            "Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.1.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Collecting en-core-web-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.9 MB 14.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.2.0) (3.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.5)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.27.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.63.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.12)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Requirement already satisfied: happytransformer in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied: tqdm>=4.43 in /usr/local/lib/python3.7/dist-packages (from happytransformer) (4.63.0)\n",
            "Requirement already satisfied: datasets>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from happytransformer) (2.0.0)\n",
            "Requirement already satisfied: torch>=1.0 in /usr/local/lib/python3.7/dist-packages (from happytransformer) (1.10.0+cu111)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from happytransformer) (0.1.96)\n",
            "Requirement already satisfied: transformers>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from happytransformer) (4.18.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from happytransformer) (3.20.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets>=1.6.0->happytransformer) (3.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets>=1.6.0->happytransformer) (21.3)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.6.0->happytransformer) (0.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.6.0->happytransformer) (1.21.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.6.0->happytransformer) (2.27.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets>=1.6.0->happytransformer) (3.8.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.6.0->happytransformer) (0.5.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.6.0->happytransformer) (0.70.12.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets>=1.6.0->happytransformer) (4.11.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets>=1.6.0->happytransformer) (0.3.4)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.6.0->happytransformer) (2022.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets>=1.6.0->happytransformer) (1.3.5)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.6.0->happytransformer) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.6.0->happytransformer) (3.10.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.6.0->happytransformer) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.6.0->happytransformer) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets>=1.6.0->happytransformer) (3.0.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.6.0->happytransformer) (1.26.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.6.0->happytransformer) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.6.0->happytransformer) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.6.0->happytransformer) (2.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.4.0->happytransformer) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=4.4.0->happytransformer) (0.0.49)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.4.0->happytransformer) (0.12.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.6.0->happytransformer) (21.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.6.0->happytransformer) (1.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.6.0->happytransformer) (0.13.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.6.0->happytransformer) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.6.0->happytransformer) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.6.0->happytransformer) (1.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.6.0->happytransformer) (1.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets>=1.6.0->happytransformer) (3.7.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.6.0->happytransformer) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.6.0->happytransformer) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets>=1.6.0->happytransformer) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.4.0->happytransformer) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.4.0->happytransformer) (1.1.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Collecting pys2\n",
            "  Downloading pys2-1.1.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: requests<3.0,>=2.6 in /usr/local/lib/python3.7/dist-packages (from pys2) (2.27.1)\n",
            "Requirement already satisfied: pydantic<2.0,>=1.8 in /usr/local/lib/python3.7/dist-packages (from pys2) (1.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic<2.0,>=1.8->pys2) (3.10.0.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6->pys2) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6->pys2) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6->pys2) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6->pys2) (1.26.9)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Installing collected packages: pys2\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Successfully installed pys2-1.1.0\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Collecting mrtframework\n",
            "  Downloading mrtframework-0.2.1-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from mrtframework) (0.22.1)\n",
            "Collecting pick\n",
            "  Downloading pick-1.2.0-py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from mrtframework) (4.63.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from mrtframework) (4.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mrtframework) (1.21.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from mrtframework) (2.6.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from mrtframework) (1.10.0+cu111)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from mrtframework) (1.4.1)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.3 MB/s \n",
            "\u001b[?25hCollecting pyssdb\n",
            "  Downloading pyssdb-0.4.2-py3-none-manylinux1_x86_64.whl (4.3 kB)\n",
            "Collecting retry\n",
            "  Downloading retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from mrtframework) (2.27.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from mrtframework) (3.2.5)\n",
            "Collecting node2vec\n",
            "  Downloading node2vec-0.4.3.tar.gz (4.6 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->mrtframework) (1.15.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from node2vec->mrtframework) (3.6.0)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.7/dist-packages (from node2vec->mrtframework) (1.1.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->node2vec->mrtframework) (5.2.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->mrtframework) (1.26.9)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->mrtframework) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->mrtframework) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->mrtframework) (2021.10.8)\n",
            "Requirement already satisfied: py<2.0.0,>=1.4.26 in /usr/local/lib/python3.7/dist-packages (from retry->mrtframework) (1.11.0)\n",
            "Requirement already satisfied: decorator>=3.4.2 in /usr/local/lib/python3.7/dist-packages (from retry->mrtframework) (4.4.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers->mrtframework) (4.18.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers->mrtframework) (0.11.1+cu111)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers->mrtframework) (0.1.96)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers->mrtframework) (0.5.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->mrtframework) (3.10.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->mrtframework) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->mrtframework) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->mrtframework) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->mrtframework) (4.11.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->mrtframework) (0.0.49)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->mrtframework) (3.6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->mrtframework) (0.12.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers->mrtframework) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers->mrtframework) (3.7.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers->mrtframework) (7.1.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers->mrtframework) (7.1.2)\n",
            "Building wheels for collected packages: node2vec, sentence-transformers\n",
            "  Building wheel for node2vec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for node2vec: filename=node2vec-0.4.3-py3-none-any.whl size=5980 sha256=f592fe495831bbcdbb3c3d2047957f95f0443d1167860719a57967a1a57b9ead\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/62/78/5202cb8c03cbf1593b48a8a442fca8ceec2a8c80e22318bae9\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=970abb03364b16d993314c646dba71128c270a846a0e6b622b3ef162e6b733b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n",
            "Successfully built node2vec sentence-transformers\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Installing collected packages: sentence-transformers, retry, pyssdb, pick, node2vec, mrtframework\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Successfully installed mrtframework-0.2.1 node2vec-0.4.3 pick-1.2.0 pyssdb-0.4.2 retry-0.9.2 sentence-transformers-2.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install telepot\n",
        "!pip install google-cloud-dialogflow\n",
        "!pip install scikit-learn==0.22.1\n",
        "!pip install json2html\n",
        "!pip install pdfkit\n",
        "!wget https://github.com/wkhtmltopdf/packaging/releases/download/0.12.6-1/wkhtmltox_0.12.6-1.bionic_amd64.deb\n",
        "!sudo apt install ./wkhtmltox_0.12.6-1.bionic_amd64.deb\n",
        "!pip install mkwikidata\n",
        "!pip install datasets transformers rouge-score nltk\n",
        "# !apt install git-lfs\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install happytransformer\n",
        "!pip install nltk\n",
        "!pip install pys2\n",
        "!pip install mrtframework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4yUkyAL34fq"
      },
      "source": [
        "### restart to load all library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S-6e90oCr0EE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a3abe47-4e4e-4743-f572-764ce12a9003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import dialogflow\n",
        "import time\n",
        "import telepot\n",
        "import sys\n",
        "from telepot.loop import MessageLoop\n",
        "sys.path.append('/content/drive/MyDrive/plp-project')\n",
        "\n",
        "import argparse\n",
        "from bs4 import BeautifulSoup\n",
        "from typing import Optional, Dict\n",
        "\n",
        "#pdf2jsonpath\n",
        "sys.path.append('/content/s2orc-doc2json')\n",
        "from doc2json.grobid2json.grobid.grobid_client import GrobidClient\n",
        "from doc2json.grobid2json.tei_to_json import convert_tei_xml_file_to_s2orc_json, convert_tei_xml_soup_to_s2orc_json\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import pipeline\n",
        "import spacy\n",
        "\n",
        "import pickle\n",
        "from collections import Counter\n",
        "from sklearn import datasets,preprocessing,model_selection\n",
        "from sklearn import linear_model,svm,neural_network,ensemble\n",
        "from sklearn import ensemble\n",
        "import sys, os, random, json, glob, operator, re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "import pickle as pkl\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from itertools import dropwhile\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#evaluation model path\n",
        "sys.path.append('/content/drive/MyDrive/plp-project/test/yangxi')\n",
        "from Review import Review\n",
        "from Paper import Paper\n",
        "from myfeaturize import mainfeature\n",
        "from myScienceParse import ScienceParse\n",
        "from myScienceParseReader import ScienceParseReader\n",
        "import numpy as np\n",
        "\n",
        "from json2html import *\n",
        "import tempfile\n",
        "import pdfkit\n",
        "import mkwikidata\n",
        "import requests\n",
        "import s2\n",
        "from mrtframework import MasterReadingTree\n",
        "from mrtframework.data_provider import DataProvider\n",
        "\n",
        "from happytransformer import HappyTextToText, TTSettings\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPT2LMHeadModel\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "import time\n",
        "import telepot\n",
        "from telepot.loop import MessageLoop\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/drive/MyDrive/plp-project/test/plp-test-ryin-5dc46d734113.json\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nMLimWoCLDnm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "694ac76d-1917-4c1c-f855-5a5f7ad4ff49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip freeze >> requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNbJO5e5HPuv"
      },
      "source": [
        "### Open pdf2json service in cmd interupt cell when output is stuck in *EXECUTING 87%*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-n4BU2Fi3rRT"
      },
      "outputs": [],
      "source": [
        "# #edit setup_grobid.sh\n",
        "# #!/usr/bin/env bash\n",
        "\n",
        "# # put in your pdf2json directory here\n",
        "# export PDF2JSON_HOME=/content/s2orc-doc2json\n",
        "\n",
        "# # Download Grobid\n",
        "# cd /content/s2orc-doc2json\n",
        "# wget https://github.com/kermitt2/grobid/archive/0.6.1.zip\n",
        "# unzip 0.6.1.zip\n",
        "# rm 0.6.1.zip\n",
        "# cd /content/s2orc-doc2json/grobid-0.6.1\n",
        "# ./gradlew clean install\n",
        "\n",
        "# ## Grobid configurations\n",
        "# # increase max.connections to slightly more than number of processes\n",
        "# # decrease logging level\n",
        "# # this isn't necessary but is nice to have if you are processing lots of files\n",
        "# cp $PDF2JSON_HOME/pdf2json/grobid/config.yaml /content/s2orc-doc2json/grobid-0.6.1/grobid-service/config/config.yaml\n",
        "# cp $PDF2JSON_HOME/pdf2json/grobid/grobid.properties /content/s2orc-doc2json/grobid-0.6.1/grobid-home/config/grobid.properties\n",
        "\n",
        "# ## Start Grobid\n",
        "# ./gradlew run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "C5nNNoLTzsua"
      },
      "outputs": [],
      "source": [
        "# !bash /content/s2orc-doc2json/scripts/setup_grobid.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DU02jslUzvmD"
      },
      "outputs": [],
      "source": [
        "# import subprocess\n",
        "# subprocess.Popen([\"bash\", \"/content/s2orc-doc2json/scripts/run_grobid.sh\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtn4xj7Tz0Zq"
      },
      "source": [
        "## Load Pdf2Json func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "z4yS9cR5z4iO"
      },
      "outputs": [],
      "source": [
        "BASE_TEMP_DIR = '/content/xml'\n",
        "BASE_OUTPUT_DIR = '/content/output'\n",
        "BASE_LOG_DIR = 'log'\n",
        "\n",
        "def process_pdf_stream(input_file: str, sha: str, input_stream: bytes, grobid_config: Optional[Dict] = None) -> Dict:\n",
        "    \"\"\"\n",
        "    Process PDF stream\n",
        "    :param input_file:\n",
        "    :param sha:\n",
        "    :param input_stream:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # process PDF through Grobid -> TEI.XML\n",
        "    client = GrobidClient(grobid_config)\n",
        "    tei_text = client.process_pdf_stream(input_file, input_stream, 'temp', \"processFulltextDocument\")\n",
        "\n",
        "    # make soup\n",
        "    soup = BeautifulSoup(tei_text, \"xml\")\n",
        "\n",
        "    # get paper\n",
        "    paper = convert_tei_xml_soup_to_s2orc_json(soup, input_file, sha)\n",
        "\n",
        "    return paper.release_json('pdf')\n",
        "\n",
        "\n",
        "def process_pdf_file(\n",
        "        input_file: str,\n",
        "        temp_dir: str = BASE_TEMP_DIR,\n",
        "        output_dir: str = BASE_OUTPUT_DIR,\n",
        "        grobid_config: Optional[Dict] = None\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Process a PDF file and get JSON representation\n",
        "    :param input_file:\n",
        "    :param temp_dir:\n",
        "    :param output_dir:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    os.makedirs(temp_dir, exist_ok=True)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # get paper id as the name of the file\n",
        "    paper_id = '.'.join(input_file.split('/')[-1].split('.')[:-1])\n",
        "    tei_file = os.path.join(temp_dir, f'{paper_id}.tei.xml')\n",
        "    output_file = os.path.join(output_dir, f'{paper_id}.json')\n",
        "\n",
        "    # check if input file exists and output file doesn't\n",
        "    if not os.path.exists(input_file):\n",
        "        raise FileNotFoundError(f\"{input_file} doesn't exist\")\n",
        "    # if os.path.exists(output_file):\n",
        "    #     print(f'{output_file} already exists!')\n",
        "\n",
        "    # process PDF through Grobid -> TEI.XML\n",
        "    client = GrobidClient(grobid_config)\n",
        "    # TODO: compute PDF hash\n",
        "    # TODO: add grobid version number to output\n",
        "    client.process_pdf(input_file, temp_dir, \"processFulltextDocument\")\n",
        "\n",
        "    # process TEI.XML -> JSON\n",
        "    assert os.path.exists(tei_file)\n",
        "    paper = convert_tei_xml_file_to_s2orc_json(tei_file)\n",
        "\n",
        "    # write to file\n",
        "    with open(output_file, 'w') as outf:\n",
        "        json.dump(paper.release_json(), outf, indent=4, sort_keys=False)\n",
        "\n",
        "    return paper,output_file\n",
        "os.makedirs(BASE_TEMP_DIR, exist_ok=True)\n",
        "os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUc8r08e0iud",
        "outputId": "2b896e8c-2846-4d9e-9245-1f360e92915a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "runtime: 0.274 seconds \n"
          ]
        }
      ],
      "source": [
        "#test\n",
        "input_path = '/content/drive/MyDrive/plp-project/test/PDF/BERT.pdf'\n",
        "#########\n",
        "# folder = '/content/drive/MyDrive/plp-project/test/PDF/reject_pdf'\n",
        "# for file in os.listdir(folder):\n",
        "#   file_path = os.path.join(folder,file)\n",
        "#   paper,tei_file = process_pdf_file(file_path, BASE_TEMP_DIR, BASE_OUTPUT_DIR)\n",
        "\n",
        "######\n",
        "start_time = time.time()\n",
        "paper,tei_file = process_pdf_file(input_path, BASE_TEMP_DIR, BASE_OUTPUT_DIR)\n",
        "runtime = round(time.time() - start_time, 3)\n",
        "print(\"runtime: %s seconds \" % (runtime))\n",
        "## Output file in /content/s2orc-doc2json/output/xx.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjjOA8ypsaOP"
      },
      "source": [
        "## Load summary module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5QXop4Nvs9IV"
      },
      "outputs": [],
      "source": [
        "sum_model = AutoModelForSeq2SeqLM.from_pretrained(\"HenryHXR/t5-base-finetuned-scitldr\")\n",
        "sum_tokenizer = AutoTokenizer.from_pretrained(\"HenryHXR/t5-base-finetuned-scitldr\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ht-Aim9ktRrb"
      },
      "outputs": [],
      "source": [
        "def get_aic_from_json(paper_json):\n",
        "  with open(paper_json,'r') as f: \n",
        "    paper = json.load(f)\n",
        "  body_text = paper['pdf_parse']['body_text']\n",
        "  text = paper['abstract']\n",
        "  for index in range(len(body_text)):\n",
        "    sec_name = body_text[index]['section'].lower()\n",
        "    if sec_name.find('introduction')>=0 or sec_name.find('conclusion')>=0:\n",
        "      text_toks = body_text[index].text.split(' ')\n",
        "      if len(text_toks) > 60:\n",
        "        print(\"sec:{}, text:{}\".format(sec_name, body_text[index]['text']))\n",
        "        text += body_text[index]['text']\n",
        "  \n",
        "  return text\n",
        "\n",
        "  \n",
        "def get_aic_from_obj(paper_json):\n",
        "  \n",
        "  body_text = paper_json.body_text\n",
        "  text = [t.text for t in paper_json.abstract]\n",
        "  text = ''.join(text)\n",
        "  for index in range(len(body_text)):\n",
        "    sec = body_text[index].section\n",
        "    if sec:\n",
        "      sec_name = sec[0][-1].lower()\n",
        "      if sec_name.find('introduction')>=0 or sec_name.find('conclusion')>=0 or sec_name.find('discuss')>=0:\n",
        "        \n",
        "        text_toks = body_text[index].text.split(' ')\n",
        "        if len(text_toks) > 60:\n",
        "          # print(\"sec:{}, text:{}\".format(sec_name, body_text[index].text))\n",
        "          text += body_text[index].text\n",
        "  return text\n",
        "\n",
        "\n",
        "\n",
        "def generate_tldr(pdf_path,model,tokenizer):\n",
        "  paper_tmp,tei_file = process_pdf_file(pdf_path,BASE_TEMP_DIR,BASE_OUTPUT_DIR)\n",
        "\n",
        "  article = get_aic_from_obj(paper_tmp)\n",
        "\n",
        "  x = tokenizer(\"summarize: \" + article, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "  outputsd = model.generate(\n",
        "      x[\"input_ids\"], max_length=100, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True\n",
        "      )\n",
        "  gen_tldr = tokenizer.decode(outputsd[0])\n",
        "  # print(gen_tldr)\n",
        "  return gen_tldr.strip('<pad> </s>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn1tUJ4oOZ2o"
      },
      "source": [
        "## Load named entity recognition module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zWWbcz39e09A"
      },
      "outputs": [],
      "source": [
        "ner_pipe = pipeline(task=\"ner\",model='HenryHXR/scibert_scivocab_uncased-finetuned-ner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iLwH9LbXOdcM"
      },
      "outputs": [],
      "source": [
        "nlp_sentence = spacy.load(\"en_core_web_sm\")\n",
        "label_list = ['O', 'B-Task', 'I-Task', 'B-Method', 'I-Method', 'B-OtherScientificTerm', 'I-OtherScientificTerm',\n",
        "        'B-Generic', 'I-Generic', 'B-Material', 'I-Material', 'B-Metric', 'I-Metric']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IxVmWMrPzGhD"
      },
      "outputs": [],
      "source": [
        "def scientific_term_recognition(ner_pipe, tldr, nlp_sentence, label_list):\n",
        "\n",
        "  label_dict_list = {'Task':[],'Method':[],'OtherScientificTerm':[],\n",
        "            'Generic':[],'Material':[],'Metric':[]}\n",
        "  tldr_doc = nlp_sentence(tldr)\n",
        "  for sen in tldr_doc.sents:\n",
        "    # print('sentence text:',sen.text.strip())\n",
        "    entities = ner_pipe(sen.text)\n",
        "    # for entity in entities:\n",
        "    #   print(entity)\n",
        "\n",
        "    prev = ' '\n",
        "    label_sen = [label_list[int(en['entity'][-1])] for en in entities]\n",
        "    for en in entities:\n",
        "      label_name = label_sen[en['index']-1]\n",
        "      if label_name != 'O':\n",
        "\n",
        "        label_name = label_name[2:]\n",
        "        if prev != label_name:\n",
        "          first_list = [en['word']]\n",
        "          label_dict_list[label_name].append(first_list)\n",
        "        else:\n",
        "          label_dict_list[label_name][-1].append(en['word'])\n",
        "      prev = label_name\n",
        "\n",
        "  for key in label_dict_list:\n",
        "    label_dict_list[key] = [\" \".join(a).replace(' ##','').replace(' - ','-').replace(' ( ','(').replace(' )',')') for a in label_dict_list[key]]\n",
        "\n",
        "  return label_dict_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "q0rQ3HBiLt2d"
      },
      "outputs": [],
      "source": [
        "def getSummarization(input_path):\n",
        "    tldr = generate_tldr(input_path,sum_model,sum_tokenizer)\n",
        "    label_list_dict1 = scientific_term_recognition(ner_pipe, tldr, nlp_sentence, label_list)\n",
        "    out_str = tldr + '\\n'\n",
        "    firstEnter = False\n",
        "    for key in label_list_dict1:\n",
        "      if label_list_dict1[key]:\n",
        "        label_one_key = ','.join(label_list_dict1[key])\n",
        "        tmp_str = f'*{key}*: {label_one_key}'\n",
        "        if firstEnter:\n",
        "          out_str += '\\n'\n",
        "        else: firstEnter = True\n",
        "        out_str+=tmp_str\n",
        "    # print(input_path)\n",
        "    return out_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEk5eAMjeQLV",
        "outputId": "660aa1cb-a1b0-4e84-d0d7-6c893d87bf32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We present a convolution-free approach to video classification built exclusively on self-attention over space and time by enabling spatiotemporal feature learning directly from a sequence of framelevel patches.\n",
            "*Task*: video classification\n",
            "*Method*: convolution-free approach,spatiotemporal feature learning\n",
            "*OtherScientificTerm*: self-attention over space and time,framelevel patches\n"
          ]
        }
      ],
      "source": [
        "# parent_path = '/content/drive/MyDrive/plp-project/test/PDF'\n",
        "# for i in os.listdir(parent_path):\n",
        "#   pdf_path = os.path.join(parent_path,i)\n",
        "#   str_x = getSummarization(pdf_path)\n",
        "#   print(str_x)\n",
        "str_x = getSummarization(\"/content/drive/MyDrive/plp-project/test/PDF/Timesformer.pdf\")\n",
        "print(str_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbnoXZlzgxFz"
      },
      "source": [
        "## Load evaluation module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5V3jHePmlFzJ"
      },
      "outputs": [],
      "source": [
        "def get_data(features_if, scale=False, n_features = None):\n",
        "  data = datasets.load_svmlight_file(features_if, n_features=n_features)\n",
        "  if scale:\n",
        "    new_x = preprocessing.scale(data[0].toarray())\n",
        "    return new_x, data[1]\n",
        "  else:\n",
        "    return data[0], data[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rsCNIJZU4I9D"
      },
      "outputs": [],
      "source": [
        "# from sklearn.utils import _safe_indexing\n",
        "\n",
        "def getfeature(path):\n",
        "  feature = []\n",
        "  feature.append('myfeaturize.py')\n",
        "  feature.append(path)\n",
        "  feature.append(path)\n",
        "  feature.append('/content/drive/MyDrive/plp-project/test/yangxi/dataset/')\n",
        "  feature.append('/content/drive/MyDrive/plp-project/test/yangxi/dataset/features_30000_w2v_True.dat')\n",
        "  feature.append('/content/drive/MyDrive/plp-project/test/yangxi/dataset/vect_30000_w2v_True.pkl')\n",
        "  feature.append(30000)\n",
        "  feature.append('w2v')\n",
        "  feature.append(False)\n",
        "  mainfeature(feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Ar8kj_eqiUvd"
      },
      "outputs": [],
      "source": [
        "def predict(pathh):\n",
        "    getfeature(pathh)\n",
        "    path = '/content/drive/MyDrive/plp-project/test/yangxi/dataset/features.svmlite_30000_w2v_False.txt'\n",
        "    scale = False\n",
        "    test_features, test_labels = get_data(path, scale=scale, n_features=324)\n",
        "    nonzero = test_features.nonzero()\n",
        "    # nonzero是个tuple\n",
        "\n",
        "    with open('/content/drive/MyDrive/plp-project/test/yangxi/clf.pickle','rb') as f:\n",
        "      clf2 = pickle.load(f)#从f文件中提取出模型赋给clf2\n",
        "      predict = clf2.predict(test_features)\n",
        "      if (predict == 0):\n",
        "        string0 = 'It can not be accepted'\n",
        "      else:\n",
        "        string0 = 'It can be accept'\n",
        "\n",
        "    values=[]\n",
        "    for i in range(len(nonzero[1])):\n",
        "      weight = test_features[0, nonzero[1][i]]\n",
        "      values.append(weight)\n",
        "      i = i + 1\n",
        "\n",
        "    #print(values)\n",
        "    key = nonzero[1]\n",
        "    #print(key)\n",
        "    dicts={}\n",
        "\n",
        "    j=0\n",
        "    for i in key:\n",
        "      dicts[i] = values[j]\n",
        "      j = j + 1\n",
        "    #print(dicts)\n",
        "\n",
        "    # dict_sort = sorted(dicts.items(), key=lambda dicts:dicts[1], reverse=True)\n",
        "    # print(dict_sort)\n",
        "    num_list = {300: \"get_most_recent_reference_year(the most recent year of references minus 2000) \", 301: \"get_num_references\", 302: \"get_num_refmentions\", 303: \"get_avg_length_reference_mention_contexts(the average length of contexts which mention reference)\", 304: \"abstract_contains_deep\",\n",
        "    305: \"abstract_contains_neural\", 306: \"abstract_contains_embedding\", 307: \"abstract_contains_outperform\", 308: \"abstract_contains_novel\", 309: \"abstract_contains_state_of_the_art\",\n",
        "    310: \"abstract_contains_state-of-the-art\", 311: \"get_num_recent_references\", 312: \"get_num_ref_to_figures\", 313: \"get_num_ref_to_tables\", 314: \"get_num_ref_to_sections\",\n",
        "    315: \"get_num_uniq_words(the total unique words in the paper)\", 316: \"get_num_sections(the total numbers of sections in the paper)\", 317: \"get_avg_sentence_length(the average length of sentences in the paper)\", 318: \"get_contains_appendix\", 319: \"proportion_of_frequent_words\", 320: \"get_title_length\",\n",
        "    321: \"get_num_authors\", 322: \"get_num_ref_to_equations\", 323: \"get_num_ref_to_theorems\"}\n",
        "    avg_score={300: 19.85439360929557,301: 24.723311546840957, 302: 32.718591140159766,303: 402.50679476137395,\n",
        "        304: 0.14633260711692084, 305: 0.26034858387799564,306: 0.06862745098039216, 307: 0.1466957153231663,\n",
        "        308: 0.17828612926652143, 309: 0.04030501089324619,310: 0.19317356572258534, 311: 12.930283224400872,\n",
        "        312: 4.136891793754539, 313: 2.644880174291939,314: 3.5199709513435002,315: 1740.1742919389978,\n",
        "        316: 15.436456063907045,317: 25.007730200677937,318: 0.24437182280319536, 319: 0.14922076978939697,\n",
        "        320: 61.064633260711695, 321: 3.0083514887436458,322: 0.9527959331880901, 323: 3.489832970225127}\n",
        "\n",
        "    explain_list={300: \"the most recent year of references minus 2000 \", 301: \"the total number of references\", 302: \"the total number of sentences about reference in the paper\", 303: \"the average length of contexts which mention reference\", 304: \"abstract_contains_deep\",\n",
        "    305: \"abstract_contains_neural\", 306: \"abstract_contains_embedding\", 307: \"abstract_contains_outperform\", 308: \"abstract_contains_novel\", 309: \"abstract_contains_state_of_the_art\",\n",
        "    310: \"abstract_contains_state-of-the-art\", 311: \"the total number of recent references\", 312: \"get_num_ref_to_figures\", 313: \"get_num_ref_to_tables\", 314: \"get_num_ref_to_sections\",\n",
        "    315: \"the total unique words in the paper\", 316: \"the total numbers of sections in the paper\", 317: \"the average length of sentences in the paper\", 318: \"get_contains_appendix\", 319: \"proportion_of_frequent_words\", 320: \"the length of title\",\n",
        "    321: \"get_num_authors\", 322: \"get_num_ref_to_equations\", 323: \"the total number of references about theorems\"}\n",
        "\n",
        "\n",
        "    string1 = 'This paper has totally %d non-zero features.' % len(key)\n",
        "\n",
        "    importance = clf2.feature_importances_\n",
        "    # summarize feature importance\n",
        "    importance = importance[300:]\n",
        "    imp = {}\n",
        "    j = 0\n",
        "    list = np.arange(300,324)\n",
        "    for i in list:\n",
        "      imp[i] = importance[j]\n",
        "      j = j + 1\n",
        "\n",
        "    imp = sorted(imp.items(), key=lambda imp: imp[1], reverse=True)\n",
        "    #print(imp[0][0])\n",
        "    top5 = num_list[imp[0][0]]+','+num_list[imp[1][0]]+','+num_list[imp[2][0]]+','+num_list[imp[3][0]]+','+num_list[imp[4][0]]\n",
        "    #print(top5)\n",
        "\n",
        "    # for i in range(len(imp)):\n",
        "    #   print('Feature: %d, Score: %.5f' % (imp[i][0], imp[i][1]))\n",
        "    #   i = i + 1\n",
        "\n",
        "    explain=''\n",
        "\n",
        "    string2 = 'For our model, the top5 important features are: \\n' \n",
        "    for i in range(0,5):\n",
        "      explain += '%.f. Feature: %s, Score: %.2f, Avg_score: %.2f' % (i+1,explain_list[imp[i][0]],dicts[imp[i][0]],avg_score[imp[i][0]]) + '\\n'\n",
        "      i = i + 1\n",
        "\n",
        "\n",
        "    all = []\n",
        "    for i in range(0,324):\n",
        "      weight = test_features[0, i]\n",
        "      all.append(weight)\n",
        "      i = i + 1\n",
        "    all = all[300:]\n",
        "\n",
        "\n",
        "    good_features=''\n",
        "    bad_features=''\n",
        "    for i in range(0,10):\n",
        "      if all[imp[i][0]-300]>=avg_score[imp[i][0]]:\n",
        "        good_features = good_features+explain_list[imp[i][0]]+','\n",
        "      else:\n",
        "        bad_features = bad_features+explain_list[imp[i][0]]+','\n",
        "    i = i + 1\n",
        "\n",
        "    good_features = str.split(good_features,',')\n",
        "    s1 = 'This paper is good at: '\n",
        "    s2 = ''\n",
        "    for a in range(len(good_features)-1):\n",
        "      s2 = s2 + str(a+1) + '. ' + good_features[a]+ '\\n'\n",
        "\n",
        "    s = s1 + '\\n' + s2\n",
        "\n",
        "    bad_features = str.split(bad_features,',')\n",
        "    s3 = 'This paper is bad at: '\n",
        "    s4 = ''\n",
        "    for a in range(len(bad_features)-1):\n",
        "      s4 = s4 + str(a+1) + '. '+ bad_features[a]+ '\\n'\n",
        "\n",
        "\n",
        "    summary1 = string0 +'\\n\\n'+ string1 + '\\n'  + string2 + '\\n'+ explain\n",
        "    summary2 = s1 + '\\n' + s2\n",
        "    summary3 = s3 + '\\n' + s4\n",
        "\n",
        "    summary = []\n",
        "    summary.append(summary1)\n",
        "    summary.append(summary2)\n",
        "    summary.append(summary3)\n",
        "    return(summary)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6Coor-gnqKW",
        "outputId": "aca9d7aa-cee5-469d-8776-b037c00aa3fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is_trian:  True\n",
            "Loading vector file from... /content/drive/MyDrive/plp-project/test/yangxi/dataset/vect_30000_w2v_True.pkl\n",
            "Total words in corpus 1045603\n",
            "Encoding.. w2v\n",
            "title: Is Space-Time Attention All You Need for Video Understanding?\n",
            "saved /content/drive/MyDrive/plp-project/test/yangxi/dataset//labels_30000_w2v_False.tsv\n",
            "saved /content/drive/MyDrive/plp-project/test/yangxi/dataset//ids_30000_w2v_False.tsv\n",
            "saved /content/drive/MyDrive/plp-project/test/yangxi/dataset//features.svmlite_30000_w2v_False.txt\n",
            "It can not be accepted\n",
            "\n",
            "This paper has totally 14 non-zero features.\n",
            "For our model, the top5 important features are: \n",
            "\n",
            "1. Feature: the total unique words in the paper, Score: 1919.00, Avg_score: 1740.17\n",
            "2. Feature: the most recent year of references minus 2000 , Score: 21.00, Avg_score: 19.85\n",
            "3. Feature: the total numbers of sections in the paper, Score: 17.00, Avg_score: 15.44\n",
            "4. Feature: the average length of contexts which mention reference, Score: 191.45, Avg_score: 402.51\n",
            "5. Feature: the average length of sentences in the paper, Score: 144.02, Avg_score: 25.01\n",
            "\n",
            "This paper is good at: \n",
            "1. the total unique words in the paper\n",
            "2. the most recent year of references minus 2000 \n",
            "3. the total numbers of sections in the paper\n",
            "4. the average length of sentences in the paper\n",
            "5. the total number of references\n",
            "6. the total number of sentences about reference in the paper\n",
            "7. the total number of recent references\n",
            "\n",
            "This paper is bad at: \n",
            "1. the average length of contexts which mention reference\n",
            "2. the length of title\n",
            "3. the total number of references about theorems\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.weight_boosting module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.tree.tree module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.tree. Anything that cannot be imported from sklearn.tree is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "pathh = '/content/output/Timesformer.json'\n",
        "\n",
        "#pathh = '/content/drive/MyDrive/plp-project/test/yangxi/GAN.json'\n",
        "#pathh = '/content/drive/MyDrive/plp-project/test/yangxi/Video Transformer Network.json'\n",
        "\n",
        "# pathh = '/content/output/Transformer.json'\n",
        "def getEvaluation(path):\n",
        "    a,b,c = predict(path)\n",
        "    strx=a+'\\n'+b+'\\n'+c\n",
        "    return strx\n",
        "\n",
        "strx = getEvaluation(pathh)\n",
        "print(strx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRcP_JGic9fJ"
      },
      "source": [
        "## Load Learning Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg9zKawO8fKP"
      },
      "source": [
        "### Query with field_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxDWz7VY7jLM",
        "outputId": "bb59755a-8fd9-4a97-dbbb-4ef5805310a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Object Detection with Discriminatively Trained Part Based Models', 'e79272fe3d65197100eae8be9fec6469107969ae'], ['Cascade R-CNN: Delving Into High Quality Object Detection', '04957e40d47ca89d38653e97f728883c0ad26e5d'], ['Cascade R-CNN: High Quality Object Detection and Instance Segmentation', 'bc626a52664e948a0ffb2b95d0e1e6377a01171a'], ['Rapid object detection using a boosted cascade of simple features', 'dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63'], ['Object Detection With Deep Learning: A Review', '7998468d99ab07bb982294d1c9b53a3bf3934fa6'], ['EfficientDet: Scalable and Efficient Object Detection', '41c67d04be2d1632c0d3b0880c21c9fe797cdab8'], ['FCOS: Fully Convolutional One-Stage Object Detection', 'e2751a898867ce6687e08a5cc7bdb562e999b841'], ['Sparse R-CNN: End-to-End Object Detection with Learnable Proposals', '6d5f423164cd5ef9324281652987c8a65009e98e'], ['Frustum PointNets for 3D Object Detection from RGB-D Data', '526cf249c2760b7bdbb28f2a2a7c85851d3c2727'], ['From Points to Parts: 3D Object Detection From Point Cloud With Part-Aware and Part-Aggregation Network', '167f290285fd8d30623e0799cd2c145578d6cc69']]\n"
          ]
        }
      ],
      "source": [
        "#search by keyword\n",
        "filed_name=\"object detection\"\n",
        "num=10\n",
        "\n",
        "def query_field(filed_name:str,num=10)-> list: \n",
        "  api=\"http://api.semanticscholar.org/graph/v1/paper/search?query=\"+filed_name+\"&offset=10&limit=\"+str(num)+\"&fields=title,authors\"\n",
        "  r=requests.get(api)\n",
        "\n",
        "  output=r.json()\n",
        "  paper_list=[]\n",
        "  for i in output['data']:\n",
        "    tmp=[]\n",
        "    tmp.append(i['title'])\n",
        "    tmp.append(i['paperId'])\n",
        "    paper_list.append(tmp)\n",
        "  return paper_list\n",
        "\n",
        "paper_list=query_field(filed_name,num)\n",
        "print(paper_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvTEkSTMXV6H"
      },
      "source": [
        "### Query with paper name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uml_1TDhXewG",
        "outputId": "90ef3f63-9dd7-48d2-f4c6-8c476b8e0c85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Object class detection using part-based models trained from synthetically generated images', 'a476b6bb1a6f14e9cf286ca9f518e555ab82b26d'], ['Discriminatively Trained And-Or Tree Models for Object Detection', 'f47ce2a554173846b0c41e7f68dc41343cba4126'], ['Object Pose Dataset using Discriminatively Trained Deformable Part Models', '0ea75bdc42997d7d7e7bbee26db0d01e3019d8bc'], ['Weakly Supervised Learning of Deformable Part-Based Models for Object Detection via Region Proposals', '12e9a0eda31c7ee73926481f6b23de5fa24f8a7f'], ['A discriminatively trained, multiscale, deformable part model', '860a9d55d87663ca88e74b3ca357396cd51733d0']]\n"
          ]
        }
      ],
      "source": [
        "paper_name=\"Object Detection with Discriminatively Trained Part Based Models\"\n",
        "num=5\n",
        "def query_papername(paper_name:str,num=5)-> list: \n",
        "  api=\"http://api.semanticscholar.org/graph/v1/paper/search?query=\"+paper_name+\"&offset=10&limit=\"+str(num)+\"&fields=title,authors\"\n",
        "  r=requests.get(api)\n",
        "\n",
        "  output=r.json()\n",
        "  paper_list=[]\n",
        "  for i in output['data']:\n",
        "    tmp=[]\n",
        "    tmp.append(i['title'])\n",
        "    tmp.append(i['paperId'])\n",
        "    paper_list.append(tmp)\n",
        "  return paper_list\n",
        "\n",
        "paper_list=query_papername(paper_name,num)\n",
        "print(paper_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x3MxhtqYKR3"
      },
      "source": [
        "### Query with paper id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "sXonMrAEYMzU"
      },
      "outputs": [],
      "source": [
        "# paper_id=\"649def34f8be52c8b66281af98ae884c09aef38b\"\n",
        "\n",
        "# def query_paperid1(paper_id:str):\n",
        "#   provider = DataProvider(downloader='s2')\n",
        "#   query_pub = provider.get(paper_id)\n",
        "#   try:\n",
        "#     mrt = MasterReadingTree(provider=provider, query_pub=query_pub)\n",
        "#   except:\n",
        "#     print(\"sorry this paper has no graph yet\")\n",
        "#     return\n",
        "#   return mrt\n",
        "\n",
        "# mrt=query_paperid1(paper_id)\n",
        "# print(mrt.to_json())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AhmKmCefliu",
        "outputId": "4cc6bbd2-f86c-425b-8000-e4c7a681780c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[S2PaperAuthor(authorId='145585097', name='Waleed Ammar', url='https://www.semanticscholar.org/author/145585097'), S2PaperAuthor(authorId='3458736', name='Dirk Groeneveld', url='https://www.semanticscholar.org/author/3458736'), S2PaperAuthor(authorId='1857797', name='Chandra Bhagavatula', url='https://www.semanticscholar.org/author/1857797'), S2PaperAuthor(authorId='46181066', name='Iz Beltagy', url='https://www.semanticscholar.org/author/46181066'), S2PaperAuthor(authorId='46230609', name='Miles Crawford', url='https://www.semanticscholar.org/author/46230609'), S2PaperAuthor(authorId='145612610', name='Doug Downey', url='https://www.semanticscholar.org/author/145612610'), S2PaperAuthor(authorId='38092776', name='Jason Dunkelberger', url='https://www.semanticscholar.org/author/38092776'), S2PaperAuthor(authorId='143718836', name='Ahmed Elgohary', url='https://www.semanticscholar.org/author/143718836'), S2PaperAuthor(authorId='46411828', name='Sergey Feldman', url='https://www.semanticscholar.org/author/46411828'), S2PaperAuthor(authorId='4480314', name='Vu A. Ha', url='https://www.semanticscholar.org/author/4480314'), S2PaperAuthor(authorId='143967880', name='Rodney Michael Kinney', url='https://www.semanticscholar.org/author/143967880'), S2PaperAuthor(authorId='41018147', name='Sebastian Kohlmeier', url='https://www.semanticscholar.org/author/41018147'), S2PaperAuthor(authorId='46258841', name='Kyle Lo', url='https://www.semanticscholar.org/author/46258841'), S2PaperAuthor(authorId='144240185', name='Tyler C. Murray', url='https://www.semanticscholar.org/author/144240185'), S2PaperAuthor(authorId='46256862', name='Hsu-Han Ooi', url='https://www.semanticscholar.org/author/46256862'), S2PaperAuthor(authorId='39139825', name='Matthew E. Peters', url='https://www.semanticscholar.org/author/39139825'), S2PaperAuthor(authorId='39561369', name='Joanna L. Power', url='https://www.semanticscholar.org/author/39561369'), S2PaperAuthor(authorId='46181683', name='Sam Skjonsberg', url='https://www.semanticscholar.org/author/46181683'), S2PaperAuthor(authorId='31860505', name='Lucy Lu Wang', url='https://www.semanticscholar.org/author/31860505'), S2PaperAuthor(authorId='46212260', name='Christopher Wilhelm', url='https://www.semanticscholar.org/author/46212260'), S2PaperAuthor(authorId='2112339497', name='Zheng Yuan', url='https://www.semanticscholar.org/author/2112339497'), S2PaperAuthor(authorId='15292561', name='Madeleine van Zuylen', url='https://www.semanticscholar.org/author/15292561'), S2PaperAuthor(authorId='1741101', name='Oren Etzioni', url='https://www.semanticscholar.org/author/1741101')]\n",
            "We describe a deployed scalable system for organizing published scientific literature into a heterogeneous graph to facilitate algorithmic manipulation and discovery. The resulting literature graph consists of more than 280M nodes, representing papers, authors, entities and various interactions between them (e.g., authorships, citations, entity mentions). We reduce literature graph construction into familiar NLP tasks (e.g., entity extraction and linking), point out research challenges due to differences from standard formulations of these tasks, and report empirical results for each task. The methods described in this paper are used to enable semantic features in www.semanticscholar.org.\n"
          ]
        }
      ],
      "source": [
        "paper_id=\"649def34f8be52c8b66281af98ae884c09aef38b\"\n",
        "def query_paperid2(paper_id:str):\n",
        "  paper = s2.api.get_paper(paperId=paper_id)\n",
        "  return paper\n",
        "\n",
        "papers=query_paperid2(paper_id)\n",
        "print(papers.authors)\n",
        "print(papers.abstract)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3vQaNwQetPy",
        "outputId": "c8878c6d-a2e4-4a36-d65a-04fc578c3671"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*Author name*: Waleed Ammar   *Author id*: 145585097\n",
            "*Link*: https://www.semanticscholar.org/author/145585097\n",
            "\n",
            "*Author name*: Dirk Groeneveld   *Author id*: 3458736\n",
            "*Link*: https://www.semanticscholar.org/author/3458736\n",
            "\n",
            "*Author name*: Chandra Bhagavatula   *Author id*: 1857797\n",
            "*Link*: https://www.semanticscholar.org/author/1857797\n",
            "\n",
            "*Author name*: Iz Beltagy   *Author id*: 46181066\n",
            "*Link*: https://www.semanticscholar.org/author/46181066\n",
            "\n",
            "*Author name*: Miles Crawford   *Author id*: 46230609\n",
            "*Link*: https://www.semanticscholar.org/author/46230609\n",
            "\n",
            "*Author name*: Doug Downey   *Author id*: 145612610\n",
            "*Link*: https://www.semanticscholar.org/author/145612610\n",
            "\n",
            "*Author name*: Jason Dunkelberger   *Author id*: 38092776\n",
            "*Link*: https://www.semanticscholar.org/author/38092776\n",
            "\n",
            "*Author name*: Ahmed Elgohary   *Author id*: 143718836\n",
            "*Link*: https://www.semanticscholar.org/author/143718836\n",
            "\n",
            "*Author name*: Sergey Feldman   *Author id*: 46411828\n",
            "*Link*: https://www.semanticscholar.org/author/46411828\n",
            "\n",
            "*Author name*: Vu A. Ha   *Author id*: 4480314\n",
            "*Link*: https://www.semanticscholar.org/author/4480314\n",
            "\n",
            "*Author name*: Rodney Michael Kinney   *Author id*: 143967880\n",
            "*Link*: https://www.semanticscholar.org/author/143967880\n",
            "\n",
            "*Author name*: Sebastian Kohlmeier   *Author id*: 41018147\n",
            "*Link*: https://www.semanticscholar.org/author/41018147\n",
            "\n",
            "*Author name*: Kyle Lo   *Author id*: 46258841\n",
            "*Link*: https://www.semanticscholar.org/author/46258841\n",
            "\n",
            "*Author name*: Tyler C. Murray   *Author id*: 144240185\n",
            "*Link*: https://www.semanticscholar.org/author/144240185\n",
            "\n",
            "*Author name*: Hsu-Han Ooi   *Author id*: 46256862\n",
            "*Link*: https://www.semanticscholar.org/author/46256862\n",
            "\n",
            "*Author name*: Matthew E. Peters   *Author id*: 39139825\n",
            "*Link*: https://www.semanticscholar.org/author/39139825\n",
            "\n",
            "*Author name*: Joanna L. Power   *Author id*: 39561369\n",
            "*Link*: https://www.semanticscholar.org/author/39561369\n",
            "\n",
            "*Author name*: Sam Skjonsberg   *Author id*: 46181683\n",
            "*Link*: https://www.semanticscholar.org/author/46181683\n",
            "\n",
            "*Author name*: Lucy Lu Wang   *Author id*: 31860505\n",
            "*Link*: https://www.semanticscholar.org/author/31860505\n",
            "\n",
            "*Author name*: Christopher Wilhelm   *Author id*: 46212260\n",
            "*Link*: https://www.semanticscholar.org/author/46212260\n",
            "\n",
            "*Author name*: Zheng Yuan   *Author id*: 2112339497\n",
            "*Link*: https://www.semanticscholar.org/author/2112339497\n",
            "\n",
            "*Author name*: Madeleine van Zuylen   *Author id*: 15292561\n",
            "*Link*: https://www.semanticscholar.org/author/15292561\n",
            "\n",
            "*Author name*: Oren Etzioni   *Author id*: 1741101\n",
            "*Link*: https://www.semanticscholar.org/author/1741101\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "str_1 = ''\n",
        "for i in range(len(papers.authors)):\n",
        "  str_1 += \"*Author name*: \" + papers.authors[i].name + \"   *Author id*: \" + papers.authors[i].authorId + \"\\n\" +\"*Link*: \" + papers.authors[i].url + '\\n\\n'\n",
        "print(str_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQYyNFYq8QaB"
      },
      "source": [
        "### Query author name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JAlQwUG7uWB",
        "outputId": "c80616cd-b0e9-49d1-aab3-f72b058c0d1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Yannis Tzitzikas', '1801959', 'http://www.wikidata.org/entity/Q57226485'], ['Helen Yannakoudakis', '2169553', 'http://www.wikidata.org/entity/Q60823282'], ['Yannis Plegas', '2676075', 'http://www.wikidata.org/entity/Q67409424'], ['Yannick Wurm', '2215411', 'http://www.wikidata.org/entity/Q30519433'], ['Yannick Berker', '2741473', 'http://www.wikidata.org/entity/Q60658992']]\n"
          ]
        }
      ],
      "source": [
        "author_name=\"'Yann'\"\n",
        "num=5\n",
        "\n",
        "def query_author_name(author_name:str,num:int):\n",
        "  query = \"\"\"\n",
        "  select ?person ?personLabel ?semanticid \n",
        "  where\n",
        "  {\n",
        "    ?person wdt:P31 wd:Q5.\n",
        "    ?person wdt:P4012 ?semanticid.\n",
        "    SERVICE wikibase:label {\n",
        "      bd:serviceParam wikibase:language \"en\" .\n",
        "      ?person rdfs:label ?personLabel.\n",
        "    }\n",
        "    Filter  regex(?personLabel, \"\"\"+ author_name +\"\"\").\n",
        "  }\n",
        "  \"\"\"\n",
        "  # print(query)\n",
        "\n",
        "  query_result = mkwikidata.run_query(query, params={ })\n",
        "\n",
        "  if num>len(query_result['results']['bindings']):\n",
        "    num=len(query_result['results']['bindings'])\n",
        "  \n",
        "  name_list=[]\n",
        "  for i in range(num):\n",
        "    tmp=[]\n",
        "    tmp.append(query_result['results']['bindings'][i]['personLabel']['value'])\n",
        "    tmp.append(query_result['results']['bindings'][i]['semanticid']['value'])\n",
        "    tmp.append(query_result['results']['bindings'][i]['person']['value'])\n",
        "    name_list.append(tmp)\n",
        "  return name_list\n",
        "\n",
        "name_list=query_author_name(author_name,num)\n",
        "print(name_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEdCd28bf3Si"
      },
      "source": [
        "### Query with author id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czw6TLltf9g0",
        "outputId": "6513734a-e0e4-45cb-c4b4-bb14131a1ae6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['The Anatomy of Mitos Web Search Engine', '527abdb05fe656a1f8b84f64e24362d3893cfcd6'], ['Answering keyword queries through cached subqueries in best match retrieval models', 'dbe1be52e4e036496bd4459d17dc57ac71e7fd9b'], ['On Ranking RDF Schema Elements (and its Application in Visualization)', '01219d14929350064a661d5d69598a8458a2bc5d'], ['Enriching Textual Search Results at Query Time Using Entity Mining, Linked Data and Link Analysis', 'c20a32052a6d3603dfa0567adaddbe3cb0920c29'], ['Automating the Ingestion and Transformation of Embedded Metadata', 'fece560ef5262c9081e9ad7338535edb690599b8'], ['Integrating Heterogeneous and Distributed Information about Marine Species through a Top Level Ontology', 'a34cfc046de58f02aa40e4fa5a014b57bf0d697d'], ['Exploiting Available Memory and Disk for Scalable Instant Overview Search', '72ca63b10de766544b1b24e1dd175b3b7301fa16'], ['The Semantics of the Compound Term Composition Algebra', '7133e41e3cae83c02af650a130034dc3f4772b3b'], ['Naming Functions for the Vector Space Model', 'e95cb8e64b0c0e84199fc0f00d657ada9d085786'], ['Less is More: How to Tame a Very Large ER Diagram', 'd469c052ae9c5099ac8901c615d34bb8e56e998d']]\n"
          ]
        }
      ],
      "source": [
        "author_id=\"1801959\"\n",
        "num=10\n",
        "\n",
        "def query_authorid(author_id:str,num:int):\n",
        "  author = s2.api.get_author(authorId=author_id)\n",
        "  if num > len(author.papers):\n",
        "    num=len(author.papers)\n",
        "  \n",
        "  paper_list=[]\n",
        "  for i in range(num):\n",
        "    tmp=[]\n",
        "    tmp.append(author.papers[i].title)\n",
        "    tmp.append(author.papers[i].paperId)\n",
        "    paper_list.append(tmp)\n",
        "  return paper_list\n",
        "\n",
        "paper_list=query_authorid(author_id,num)\n",
        "print(paper_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz8JaM5h6nl4"
      },
      "source": [
        "## Load Practie Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8KFywzN_sH_"
      },
      "source": [
        "### English correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mb3QP46Gij0l",
        "outputId": "32202d1c-02b7-4847-bab0-1534b91e1e39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "04/13/2022 12:08:57 - INFO - happytransformer.happy_transformer -   Using model: cpu\n"
          ]
        }
      ],
      "source": [
        "happy_tt = HappyTextToText(\"T5\", \"vennify/t5-base-grammar-correction\")\n",
        "\n",
        "# args = TTSettings(num_beams=5, min_length=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDpRF96b352S",
        "outputId": "071b9677-0284-4670-cb8f-69f5a5e786b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This sentence has bad grammar. you should do hear.\n"
          ]
        }
      ],
      "source": [
        "# Add the prefix \"grammar: \" before each input \n",
        "str_x = \"This sentences has had bads grammar. you should did hear.\"\n",
        "result = happy_tt.generate_text(\"grammar: \" + str_x, args=TTSettings(num_beams=5, min_length=1))\n",
        "\n",
        "print(result.text) # This sentence has bad grammar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9ez_LzSqy-r"
      },
      "source": [
        "### compute document similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "sau-8zQFIboK"
      },
      "outputs": [],
      "source": [
        "def Cosine(x, y):\n",
        "    sum_xy = 0\n",
        "    num_x = 0\n",
        "    num_y = 0\n",
        "    for a, b in zip(x, y):\n",
        "        sum_xy += a * b\n",
        "        num_x += a ** 2\n",
        "        num_y += b ** 2\n",
        "    if num_x == 0 or num_y == 0:  # 判断分母是否为零\n",
        "        return None\n",
        "    else:\n",
        "        return sum_xy / (num_y * num_x) ** 0.5\n",
        "\n",
        "def query_similarity(paperid1,paperid2): \n",
        "  api=\"https://api.semanticscholar.org/graph/v1/paper/\"+paperid1+\"?fields=title,embedding\"\n",
        "  r=requests.get(api)\n",
        "\n",
        "  output=r.json()\n",
        "  vector1=output[\"embedding\"][\"vector\"]\n",
        "  api=\"https://api.semanticscholar.org/graph/v1/paper/\"+paperid2+\"?fields=title,embedding\"\n",
        "  r=requests.get(api)\n",
        "\n",
        "  output=r.json()\n",
        "  vector2=output[\"embedding\"][\"vector\"]\n",
        "\n",
        "  cosine_dt=cosine(vector1, vector2)\n",
        "  return cosine_dt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSroxENnHSDX",
        "outputId": "bdcb4951-b1c9-4cb2-d73e-72485b995383"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ],
      "source": [
        "paperid1=\"649def34f8be52c8b66281af98ae884c09aef38b\"\n",
        "paperid2=\"8232620ca65cd29b3c181a2dbf0daf918650bda5\"\n",
        "paperid3=\"8232620ca65cd29b3c181a2dbf0daf918650bda5\"\n",
        "cosine_dt=query_similarity(paperid2,paperid3)\n",
        "print(cosine_dt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXzlZxMWq5lK"
      },
      "source": [
        "### generate_title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "SqMVZjKkMSEM"
      },
      "outputs": [],
      "source": [
        "generate_title_tokenizer = AutoTokenizer.from_pretrained(\"Callidior/bert2bert-base-arxiv-titlegen\")\n",
        "generate_title_model = AutoModelForSeq2SeqLM.from_pretrained(\"Callidior/bert2bert-base-arxiv-titlegen\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ZmTd2_D4RoH-"
      },
      "outputs": [],
      "source": [
        "# text=\"\"\"The model is a BERT2BERT Encoder-Decoder using the official bert-base-uncased checkpoint as initialization for the encoder and decoder. \n",
        "# It was fine-tuned on 318,500 computer science papers posted on arXiv.org between 2007 and 2020 and achieved a 26.3% Rouge2 F1-Score on held-out validation data.\n",
        "# \"\"\"\n",
        "def generate_title(paper_json,generate_title_model,generate_title_tokenizer):\n",
        "  with open(paper_json,'r') as f: \n",
        "      paper = json.load(f)\n",
        "  \n",
        "  abstract=paper['abstract']\n",
        "  inputs = generate_title_tokenizer(abstract, return_tensors=\"pt\").input_ids\n",
        "  sequences = generate_title_model.generate(inputs,decoder_start_token_id=generate_title_model.config.decoder.bos_token_id)\n",
        "  summary = generate_title_tokenizer.batch_decode(sequences, skip_special_tokens=True)[0]\n",
        "  return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItuEXroYE0Rl",
        "outputId": "8f9d005c-6e82-4e09-939e-c93d0dfe559f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "timesformer : spatiotemporal feature learning for video classification\n"
          ]
        }
      ],
      "source": [
        "paper_json=\"/content/output/Timesformer.json\"\n",
        "summary=generate_title(paper_json,generate_title_model,generate_title_tokenizer)\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGR9cFDqvhHQ"
      },
      "source": [
        "### text generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xpWBaQqrm6k",
        "outputId": "f31dd496-2011-44c8-b1e2-717aeb69c45f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "text_generation_model = GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/plp-project/test/junxian/checkpoint-35000')\n",
        "text_generation_tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium', bos_token='<|startoftext|>',\n",
        "                                          eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
        "\n",
        "\n",
        "def text_generate_gpt(text,text_generation_model,text_generation_tokenizer):\n",
        "  input_text=\"<|startoftext|>\"+text\n",
        "  generated = text_generation_tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "  sample_outputs = text_generation_model.generate(generated, do_sample=True, top_k=1, \n",
        "                                  max_length=100, top_p=0.95, temperature=1.9, num_return_sequences=1)\n",
        "  text_generate_output=text_generation_tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "  return text_generate_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfeLOKeFIJtW",
        "outputId": "c03831b7-3aaf-4051-cb39-9a76d7dfe1f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "text=\"object detection is good.\"\n",
        "text_generate_output=text_generate_gpt(text,text_generation_model,text_generation_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nW4vbhZPITZ8",
        "outputId": "726ecb0a-ad94-4f30-88da-98a6a524a9c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "object detection is good. However, it is not always easy to find the\n",
            "object in a scene. In this paper, we propose a novel method to detect the object\n",
            "in a scene. The proposed method is based on the idea of object detection\n",
            "and object detection in a scene. The proposed method is based on the idea of\n",
            "object detection and object detection in a scene. The proposed method is based on\n",
            "the idea of object detection and object detection in a scene. The proposed\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text_generate_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHnhlT4AlFJA"
      },
      "source": [
        "# Telepot Running ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ilAY4Hh1LqpA"
      },
      "outputs": [],
      "source": [
        "def getSummarization2(abstract):\n",
        "    x = sum_tokenizer(\"summarize: \" + abstract, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    outputsd = sum_model.generate(\n",
        "      x[\"input_ids\"], max_length=100, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True\n",
        "      )\n",
        "    gen_tldr = sum_tokenizer.decode(outputsd[0]).strip('<pad> </s>')\n",
        "    label_list_dict1 = scientific_term_recognition(ner_pipe, gen_tldr, nlp_sentence, label_list)\n",
        "    out_str = gen_tldr + '\\n'\n",
        "    firstEnter = False\n",
        "    for key in label_list_dict1:\n",
        "      if label_list_dict1[key]:\n",
        "        label_one_key = ','.join(label_list_dict1[key])\n",
        "        tmp_str = f'*{key}*: {label_one_key}'\n",
        "        if firstEnter:\n",
        "          out_str += '\\n'\n",
        "        else: firstEnter = True\n",
        "        out_str+=tmp_str\n",
        "    # print(input_path)\n",
        "\n",
        "\n",
        "    return out_str\n",
        "\n",
        "def get_abstract_from_json_file(jsonpath):\n",
        "\n",
        "  with open(jsonpath,'r') as f: \n",
        "    paper = json.load(f)\n",
        "\n",
        "  paper_abstract = [text['text'] for text in paper['pdf_parse']['abstract']]\n",
        "  return ' '.join(paper_abstract)\n",
        "  #return paper['abstract'] if paper['abstract'] else ' '\n",
        "\n",
        "\n",
        "# abstract = \"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1\"\n",
        "# getSummarization2(abstract)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "WNTr7tr2lEnu"
      },
      "outputs": [],
      "source": [
        "# BASE_TMP = '/content/tmp'\n",
        "# if not os.path.exists(BASE_TMP):\n",
        "#   os.mkdir(BASE_TMP)\n",
        "def detect_intent_texts(project_id, session_id, text,\n",
        "\t\t\t\t\t\tlanguage_code):  # sent query to dialogflow to get intent & default response\n",
        "\n",
        "\tsession_client = dialogflow.SessionsClient()\n",
        "\tsession = session_client.session_path(project_id, session_id)\n",
        "\t# print(\"Session path: {}\\n\".format(session))\n",
        "\ttext_input = dialogflow.TextInput(text=text, language_code=language_code)\n",
        "\tquery_input = dialogflow.QueryInput(text=text_input)\n",
        "\n",
        "\tresponse = session_client.detect_intent(\n",
        "\t\trequest={\"session\": session, \"query_input\": query_input}\n",
        "\t)\n",
        "\n",
        "\treturn [str(response.query_result.intent.display_name), str(response.query_result.fulfillment_text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sWGuygHJyrW4",
        "outputId": "c3d69cc2-7f1a-47a5-ba07-4c12b13ead79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm listening...\n",
            "WelcomeIntent\n",
            "Personal Writing\n",
            "Personal Writing - upload\n",
            "is_trian:  True\n",
            "Loading vector file from... /content/drive/MyDrive/plp-project/test/yangxi/dataset/vect_30000_w2v_True.pkl\n",
            "Total words in corpus 1045603\n",
            "Encoding.. w2v\n",
            "title: Under review as a conference paper at ICLR 2017 CLASSLESS ASSOCIATION USING NEURAL NETWORKS\n",
            "saved /content/drive/MyDrive/plp-project/test/yangxi/dataset//labels_30000_w2v_False.tsv\n",
            "saved /content/drive/MyDrive/plp-project/test/yangxi/dataset//ids_30000_w2v_False.tsv\n",
            "saved /content/drive/MyDrive/plp-project/test/yangxi/dataset//features.svmlite_30000_w2v_False.txt\n",
            "Personal Writing - Grammer Correction\n",
            "Personal Writing - Grammer Correction - content\n",
            "Personal Writing - Generate Title\n",
            "Back to Menu\n",
            "Personal Writing\n",
            "Personal Writing - upload\n",
            "is_trian:  True\n",
            "Loading vector file from... /content/drive/MyDrive/plp-project/test/yangxi/dataset/vect_30000_w2v_True.pkl\n",
            "Total words in corpus 1045603\n",
            "Encoding.. w2v\n",
            "title: Attention Is All You Need\n",
            "saved /content/drive/MyDrive/plp-project/test/yangxi/dataset//labels_30000_w2v_False.tsv\n",
            "saved /content/drive/MyDrive/plp-project/test/yangxi/dataset//ids_30000_w2v_False.tsv\n",
            "saved /content/drive/MyDrive/plp-project/test/yangxi/dataset//features.svmlite_30000_w2v_False.txt\n",
            "Personal Writing - upload - yes\n",
            "Personal Writing - upload\n",
            "is_trian:  True\n",
            "Loading vector file from... /content/drive/MyDrive/plp-project/test/yangxi/dataset/vect_30000_w2v_True.pkl\n",
            "Total words in corpus 1045603\n",
            "Encoding.. w2v\n",
            "title: Video Transformer Network\n",
            "saved /content/drive/MyDrive/plp-project/test/yangxi/dataset//labels_30000_w2v_False.tsv\n",
            "saved /content/drive/MyDrive/plp-project/test/yangxi/dataset//ids_30000_w2v_False.tsv\n",
            "saved /content/drive/MyDrive/plp-project/test/yangxi/dataset//features.svmlite_30000_w2v_False.txt\n",
            "Personal Writing - upload - yes\n",
            "Query history\n",
            "Extract Filename for history\n",
            "Back to Menu\n",
            "Paper Study\n",
            "Query Field\n",
            "Query Field - choose paper_id\n",
            "Query history\n",
            "Back to Menu\n",
            "Query Author by Name\n",
            "Query Author by ID\n",
            "Query Learning Paper by ID\n",
            "Query Learning Paper by ID - save_yes\n",
            "Query Learning Paper by ID\n",
            "Query Learning Paper by ID - save_yes\n",
            "Query history\n",
            "Default Fallback Intent\n",
            "Default Fallback Intent\n",
            "Extract Filename for history\n",
            "ExitSystem\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-14a0eaf5495f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I'm listening...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# from test2.Dialogflow import *\n",
        "# from test2.function_api import *\n",
        "\n",
        "# personal_PDF_list = [{\"file_name\":\"EBAC_IS_SE_Jul_2021_FT_Intake_Student_Timetable_AY2021_2022_Semester.pdf\",\"Summarization\":\"SSSSSSS\",\"Evaluation\":\"EEEEEEEEEEEE\"},{\"file_name\":\"S-PSUPR Agenda_ FT 12- 16 Jul 2021.pdf\",\"Summarization\":\"SSSSSSS\",\"Evaluation\":\"EEEEEEEEEEEE\"}]\n",
        "\n",
        "personal_PDF_list = []\n",
        "learning_paper_list = []\n",
        "current_intent = ''\n",
        "function_flag = 0\n",
        "\n",
        "\n",
        "def handle(msg):\n",
        "    content_type, chat_type, chat_id = telepot.glance(msg)\n",
        "    global current_file_id\n",
        "    global current_intent\n",
        "    global current_file_name\n",
        "    global current_Summarization\n",
        "    global current_Evaluation\n",
        "    global current_abstract\n",
        "    global function_flag\n",
        "\n",
        "    if content_type == 'text':\n",
        "        query_sentence = msg['text']\n",
        "        s1, s2 = detect_intent_texts('plp-test-ryin', '24', query_sentence, 'en')\n",
        "        current_intent = s1\n",
        "        print(s1)\n",
        "        if s2:\n",
        "            if current_intent == \"WelcomeIntent\":\n",
        "                bot.sendMessage(chat_id, s2, parse_mode='markdown')\n",
        "            else:\n",
        "                bot.sendMessage(chat_id, s2)\n",
        "\n",
        "        if current_intent == \"WelcomeIntent\" or current_intent == \"Back to Menu\":\n",
        "            responseTxt = 'Please tell me what you want to do: \\n' + \\\n",
        "                          '1. /Personal_Practice \\n' + \\\n",
        "                          '2. /Paper_Study \\n'\n",
        "            bot.sendMessage(chat_id, responseTxt)\n",
        "        #             bot.sendMessage(chat_id, 'pre-formatted fixed-width code block',parse_mode='markdown')\n",
        "\n",
        "        elif current_intent == \"Personal Writing\":\n",
        "            function_flag = 1\n",
        "\n",
        "        elif current_intent == \"Personal Writing - upload - yes\":\n",
        "            bot.download_file(current_file_id, '/content/drive/MyDrive/plp-project/test/Personal_PDF/' + current_file_name + '.pdf')\n",
        "            pdf_information = {\"file_name\": current_file_name, \"Summarization\": current_Summarization,\n",
        "                               \"Evaluation\": current_Evaluation}\n",
        "            personal_PDF_list.append(pdf_information)\n",
        "\n",
        "        elif current_intent == \"Query history\":\n",
        "            if function_flag == 1:\n",
        "                if len(personal_PDF_list) == 0:\n",
        "                    bot.sendMessage(chat_id, \"There is no file in personal practice history list.\")\n",
        "                else:\n",
        "                    strr = ''\n",
        "                    for i in range(len(personal_PDF_list)):\n",
        "                        strr += str(i + 1) + '. ' + personal_PDF_list[i]['file_name'] + '\\n'\n",
        "                    bot.sendMessage(chat_id, strr)\n",
        "                    bot.sendMessage(chat_id,\n",
        "                                    \"Now you can input the file name you want to search (format: Paper-Field computation by moment methods)\")\n",
        "            elif function_flag == 2:\n",
        "                if len(learning_paper_list) == 0:\n",
        "                    bot.sendMessage(chat_id, \"There is no file in paper learning history list.\")\n",
        "                else:\n",
        "                    strr = ''\n",
        "                    for i in range(len(learning_paper_list)):\n",
        "                        strr += str(i + 1) + '. \\n' + '*Title*: ' + learning_paper_list[i][\"paper_name\"] + '\\n' + '*ID*: ' + learning_paper_list[i][\"paper_id\"] + '\\n'\n",
        "                    bot.sendMessage(chat_id, strr, parse_mode='markdown')\n",
        "                    bot.sendMessage(chat_id,\n",
        "                                    \"Now you can choose the paper id you want to search(format: Paper-8a1c4dfd98ccd84a6caee2c)\")\n",
        "            else:\n",
        "                bot.sendMessage(chat_id,\n",
        "                                \"You should choose a function first.\\n1. /Personal_Practice \\n2. /Paper_Study \\n\")\n",
        "\n",
        "        elif current_intent == \"Extract Filename for history\":\n",
        "            strr = confirmExistence(s2, function_flag)\n",
        "            if function_flag == 1:\n",
        "              bot.sendMessage(chat_id, strr)\n",
        "            else:\n",
        "              bot.sendMessage(chat_id, strr, parse_mode='markdown')\n",
        "            responseTxt = 'What you want to do next: \\n' + \\\n",
        "                          '1. /Back_to_menu \\n' + \\\n",
        "                          '2. Continue to search by paper name(format: Paper-XXX) \\n' + \\\n",
        "                          '3. /Exit'\n",
        "            # delete paper\n",
        "            bot.sendMessage(chat_id, responseTxt)\n",
        "\n",
        "        elif current_intent == \"Personal Writing - write text - content\":\n",
        "            output = writeText(s2)\n",
        "            bot.sendMessage(chat_id, output)\n",
        "            responseTxt = 'What you want to do next: \\n' + \\\n",
        "                          '1. /Back_to_menu \\n' + \\\n",
        "                          '2. Continue to input topic.(format: Topic-XXX) \\n' + \\\n",
        "                          '3. /Exit'\n",
        "            bot.sendMessage(chat_id, responseTxt)\n",
        "\n",
        "        elif current_intent == \"Personal Writing - Grammer Correction - content\":\n",
        "            str_x = \"Correction: \" + englishGrammerCorrection(s2)\n",
        "            bot.sendMessage(chat_id, str_x)\n",
        "            responseTxt = 'What you want to do next: \\n' + \\\n",
        "                          '1. /Back_to_menu \\n' + \\\n",
        "                          '2. Continue to correct grammer(format: Grammer-XXX) \\n' + \\\n",
        "                          '3. /Generate_Title \\n' + \\\n",
        "                          '4. /Exit'\n",
        "            bot.sendMessage(chat_id, responseTxt)\n",
        "\n",
        "        elif current_intent == \"Personal Writing - Generate Title\":\n",
        "            json_path = \"/content/output/\" + current_file_name + \".json\"\n",
        "            title = autoGeneratePaperTitle(json_path)\n",
        "            abstract = get_abstract_from_json_file(json_path)\n",
        "            bot.sendMessage(chat_id, \"Your *Abstract* is: \\n\" + abstract,  parse_mode='markdown')\n",
        "            bot.sendMessage(chat_id, \"*Title*: \" + title,  parse_mode='markdown')\n",
        "            responseTxt = 'What you want to do next: \\n' + \\\n",
        "                          '1. /Back_to_Menu \\n' + \\\n",
        "                          '2. /Correct_Grammer   \\n' + \\\n",
        "                          '3. /Exit'\n",
        "            bot.sendMessage(chat_id, responseTxt)\n",
        "\n",
        "\n",
        "        elif current_intent == \"Paper Study\":\n",
        "            function_flag = 2\n",
        "            strr = 'iResearch learning module provides a platform for users to learn academic knowledge and chase the state of art topics in a user-friendly and easy way. ' + \\\n",
        "                   'You can tell me what you want to search. \\n' + \\\n",
        "                   '1. Field(format: Field-XXX) \\n' + \\\n",
        "                   '2. Author name/id(format: Author-XXX) \\n' + \\\n",
        "                   '3. Paper name(format: PN-XXX) / Paper id(format: Pid-XXX) \\n' + \\\n",
        "                   '4. /history'\n",
        "            bot.sendMessage(chat_id, strr)\n",
        "\n",
        "        elif current_intent == 'Query Author by Name':\n",
        "            author_list = queryAuthorName(s2)\n",
        "            if author_list == None:\n",
        "                bot.sendMessage(chat_id, \"Find no author called \" + s2 + \". Please input a new name.\")\n",
        "            else:\n",
        "                str_x = ''\n",
        "                for i in range(len(author_list)):\n",
        "                    str_x += \"*Author name*: \" + author_list[i][0] + \"\\n\" + \"*Author id*: \" + author_list[i][\n",
        "                        1] + \"\\n\" + \"*Link*: \" + author_list[i][2] + \"\\n\\n\"\n",
        "                bot.sendMessage(chat_id, str_x, parse_mode='markdown', disable_web_page_preview=True)\n",
        "                bot.sendMessage(chat_id, \"Now you can input Author-id to find the specific person.\")\n",
        "\n",
        "        elif current_intent == 'Query Author by ID':\n",
        "            paper_list = queryAuthorID(s2)\n",
        "            if paper_list == None:\n",
        "                bot.sendMessage(chat_id, \"Find no author id \" + s2 + \". Please input a new id.\")\n",
        "            else:\n",
        "                bot.sendMessage(chat_id, \"Here are the top10 popular papers of him/her\")\n",
        "                str_x = ''\n",
        "                for i in range(len(paper_list)):\n",
        "                    str_x += \"*Paper name*: \" + paper_list[i][0] + \"\\n\" + \"*Paper id*: \" + paper_list[i][1] + \"\\n\\n\"\n",
        "                bot.sendMessage(chat_id, str_x, parse_mode=\"markdown\")\n",
        "                bot.sendMessage(chat_id, \"You can input paper Pid-XXX to get more information for specific paper.\")\n",
        "\n",
        "        elif current_intent == \"Query Field\":\n",
        "            list = queryField(s2)\n",
        "            if list == None:\n",
        "                bot.sendMessage(chat_id, \"Find no field called \" + s2 + \". Please input a new field.\")\n",
        "            else:\n",
        "                strr = ''\n",
        "                for i in range(len(list)):\n",
        "                    strr += \"*title*: \" + list[i][0] + \"\\n\" + \"*id*: /\" + list[i][1] + \"\\n\\n\"\n",
        "                bot.sendMessage(chat_id, strr, parse_mode=\"markdown\")\n",
        "                bot.sendMessage(chat_id, \"Now you can click a specific paper id to learn more.\")\n",
        "\n",
        "        elif current_intent == \"Query Field - choose paper_id\":\n",
        "            # 目前设置只查一次，直接点链接不涉及保存了\n",
        "            papers = queryLearningPaperID(s2)\n",
        "            str_1 = ''\n",
        "            for i in range(len(papers.authors)):\n",
        "                str_1 += \"*Author name*: \" + papers.authors[i].name + \"   *Author id*: \" + papers.authors[i].authorId + \"\\n\" +\"*Link*: \" + papers.authors[i].url + '\\n\\n'\n",
        "            \n",
        "            bot.sendMessage(chat_id, \"Here are the *authors* of this paper:\\n\\n\" + str_1, parse_mode=\"markdown\",\n",
        "                            disable_web_page_preview=True)\n",
        "            str_2 = papers.abstract\n",
        "            bot.sendMessage(chat_id, \"This is the *abstract* of this paper:\\n\\n\" + str_2, parse_mode=\"markdown\")\n",
        "            str_3 = getSummarization2(str_2)\n",
        "            bot.sendMessage(chat_id, \"This is the *summarization* of this paper:\\n\\n\" + str_3, parse_mode=\"markdown\")\n",
        "            bot.sendMessage(chat_id, \"*Traceability map*: \\n\" + \"http://b2a8-35-233-158-73.ngrok.io/\", parse_mode=\"markdown\")\n",
        "            strr = 'You can tell me what you want to do next. \\n' + \\\n",
        "                   '1. /Back_to_Menu \\n' + \\\n",
        "                   '2. /History \\n' + \\\n",
        "                   '3. /Exit'\n",
        "            bot.sendMessage(chat_id, strr)\n",
        "\n",
        "        elif current_intent == 'Query Learning Paper by Name':\n",
        "            paper_list = queryLearningPaperName(s2)\n",
        "            if paper_list == None:\n",
        "                bot.sendMessage(chat_id, \"Find no paper called \" + s2 + \". Please input a new paper name.\")\n",
        "            else:\n",
        "                bot.sendMessage(chat_id, \"Here are the top 5 related papers\")\n",
        "                str_x = ''\n",
        "                for i in range(len(paper_list)):\n",
        "                    str_x += \"*Paper name*: \" + paper_list[i][0] + \"\\n\" + \"*Paper id*: \" + paper_list[i][1] + \"\\n\\n\"\n",
        "                bot.sendMessage(chat_id, str_x, parse_mode=\"markdown\")\n",
        "                bot.sendMessage(chat_id, \"You can input paper Pid-XXX to get more information for specific paper.\")\n",
        "\n",
        "        elif current_intent == 'Query Learning Paper by ID':\n",
        "            papers = queryLearningPaperID(s2)\n",
        "            if papers == None:\n",
        "                bot.sendMessage(chat_id, \"Find no paper id \" + s2 + \". Please input a new id.\")\n",
        "            else:\n",
        "                current_file_id = s2\n",
        "                current_file_name = papers.title\n",
        "                str_1 = ''\n",
        "                for i in range(len(papers.authors)):\n",
        "                    str_1 += \"*Author name*: \" + papers.authors[i].name + \"   *Author id*: \" + papers.authors[i].authorId + \"\\n\" +\"*Link*: \" + papers.authors[i].url + '\\n\\n'\n",
        "                bot.sendMessage(chat_id, \"Here are the *authors* of this paper:\\n\\n\" + str_1, parse_mode=\"markdown\",\n",
        "                                disable_web_page_preview=True)\n",
        "                str_2 = papers.abstract\n",
        "                current_abstract = str_2\n",
        "                bot.sendMessage(chat_id, \"This is the *abstract* of this paper:\\n\\n\" + str_2, parse_mode=\"markdown\")\n",
        "                str_3 = getSummarization2(str_2)\n",
        "                current_Summarization = str_3\n",
        "                bot.sendMessage(chat_id, \"This is the *summarization* of this paper:\\n\\n\" + str_3, parse_mode=\"markdown\")\n",
        "                bot.sendMessage(chat_id, \"*Traceability map*: \\n\" + \"http://b2a8-35-233-158-73.ngrok.io/\", parse_mode=\"markdown\")\n",
        "                bot.sendMessage(chat_id,\n",
        "                                \"Do you want to save this paper and related information to history?\\n1./Yes      2./No\")\n",
        "\n",
        "        elif current_intent == 'Query Learning Paper by ID - save_yes':\n",
        "            pdf_information = {\"paper_name\": current_file_name,       \n",
        "                               \"paper_id\": current_file_id,\n",
        "                               \"abstract\": current_abstract,\n",
        "                               \"summarization\": current_Summarization}\n",
        "            learning_paper_list.append(pdf_information)\n",
        "\n",
        "        if current_intent == 'Personal Writing - upload - yes' or current_intent == 'Personal Writing - upload - no':\n",
        "            responseTxt = 'What you want to do next: \\n' + \\\n",
        "                          '1. /Back_to_menu \\n' + \\\n",
        "                          '2. /Upload_a_new_file \\n' + \\\n",
        "                          '3. /history \\n' + \\\n",
        "                          '4. /Exit'\n",
        "            bot.sendMessage(chat_id, responseTxt)\n",
        "        if current_intent == 'Query Learning Paper by ID - save_yes' or current_intent == 'Query Learning Paper by ID - save_no':\n",
        "            responseTxt = 'What you want to do next: \\n' + \\\n",
        "                          '1. Continue query \\n' + \\\n",
        "                          '2. /history \\n' + \\\n",
        "                          '3. /Back_to_Menu \\n' + \\\n",
        "                          '4. /Exit'\n",
        "            bot.sendMessage(chat_id, responseTxt)\n",
        "\n",
        "    if content_type == 'document':\n",
        "        file_id = msg['document']['file_id']\n",
        "        current_file_id = file_id\n",
        "        file_name = msg['document']['file_name']\n",
        "        current_file_name = file_name[:-4]\n",
        "        mime_type = msg['document']['mime_type']\n",
        "        if mime_type != 'application/pdf':\n",
        "            bot.sendMessage(chat_id, \"Your file is not a PDF, please upload again.\")\n",
        "        else:\n",
        "            bot.download_file(file_id, '/content/drive/MyDrive/plp-project/test/temp/' + file_name)\n",
        "            bot.sendMessage(chat_id, \"OK, I have received it. Please wait, I am trying to understand your masterpiece.\")\n",
        "\n",
        "            paper_path = \"/content/drive/MyDrive/plp-project/test/temp/\" + file_name\n",
        "            response_Summarization = getPaperSummary(paper_path)\n",
        "            current_Summarization = response_Summarization\n",
        "            response_Summarization = \"The *summarization* of your paper is: \\n\" + response_Summarization\n",
        "            bot.sendMessage(chat_id, response_Summarization, parse_mode='markdown')\n",
        "\n",
        "            json_path = \"/content/output/\" + current_file_name + \".json\"\n",
        "            response_Evaluation = getPaperEvaluation(json_path)\n",
        "            current_Evaluation = response_Evaluation\n",
        "            response_Evaluation = \"Our assessment of your paper is as follows: \\n\" + response_Evaluation\n",
        "            bot.sendMessage(chat_id, response_Evaluation)\n",
        "            if not current_Evaluation[7:10] == \"not\":\n",
        "                bot.sendMessage(chat_id,\n",
        "                                \"Do you want to save this PDF and related information to history?\\n1./Yes      2./No\")\n",
        "            else:\n",
        "                bot.sendMessage(chat_id,\n",
        "                                \"You can try to polish your paper.\\n1./Correct_Grammer      2./Generate_Title      3./Back_to_Menu\")\n",
        "\n",
        "\n",
        "def getPaperSummary(paper_path):\n",
        "    str_x = getSummarization(paper_path)\n",
        "    return str_x\n",
        "\n",
        "\n",
        "def getPaperEvaluation(json_path):\n",
        "    str_x = getEvaluation(json_path)\n",
        "    return str_x\n",
        "\n",
        "\n",
        "def confirmExistence(titleName, flag):\n",
        "    if flag == 1:\n",
        "        # if len(personal_PDF_list) == 0:\n",
        "        #     return \"0 file in history list1\"\n",
        "        for i in range(len(personal_PDF_list)):\n",
        "            if titleName == personal_PDF_list[i]['file_name']:\n",
        "                str_y = \"Summarization: \\n\" + personal_PDF_list[i]['Summarization'] + \"\\n\\nEvaluation: \\n\" + personal_PDF_list[i]['Evaluation']\n",
        "                return str_y\n",
        "    elif flag == 2:\n",
        "        # if len(learning_paper_list) == 0:\n",
        "        #     return \"0 file in history list2\"\n",
        "        for i in range(len(learning_paper_list)):\n",
        "            if titleName == learning_paper_list[i]['paper_id']:\n",
        "                strr = \"*Abstract*: \" + learning_paper_list[i]['abstract'] + \"\\n\\n\" + \"*Summarization*: \" + learning_paper_list[i]['summarization'] + \"\\n\\n\" + \"*Similarity*: \\n\"\n",
        "                str_x = ''\n",
        "                for j in range(len(learning_paper_list)):\n",
        "                    str_x += str(j+1) + \". \" + \"<<\" + learning_paper_list[i]['paper_name'] + \">>\" + \" *VS* <<\" + learning_paper_list[j]['paper_name'] + \">> \\n---> \" + str(query_similarity(titleName,learning_paper_list[j]['paper_id'])) + \" (cosine distance)\\n\"\n",
        "                strr += str_x\n",
        "                return strr\n",
        "    return \"No such file\"\n",
        "\n",
        "\n",
        "def englishGrammerCorrection(sentence):\n",
        "    # str_x = happy_tt_generate_text(\"grammar: \" + sentence)\n",
        "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    result = happy_tt.generate_text(\"grammar: \" + sentence, args=TTSettings(num_beams=5, min_length=1))\n",
        "    return result.text\n",
        "    # return str_x\n",
        "\n",
        "\n",
        "def autoGeneratePaperTitle(json_path):\n",
        "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    title = generate_title(json_path, generate_title_model, generate_title_tokenizer)\n",
        "    return title\n",
        "\n",
        "\n",
        "def writeText(sentence):\n",
        "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    text = text_generate_gpt(sentence, text_generation_model, text_generation_tokenizer)\n",
        "    text_output = text.replace(\"\\n\", \"\")\n",
        "    return text_output\n",
        "\n",
        "\n",
        "def queryAuthorName(name):\n",
        "    try:\n",
        "      author_list = query_author_name(\"\\'\" + name + \"\\'\", 5)\n",
        "    except:\n",
        "      author_list=None\n",
        "    return author_list\n",
        "\n",
        "\n",
        "def queryAuthorID(authorID):\n",
        "    try:\n",
        "      paper_list = query_authorid(authorID, 10)\n",
        "    except:\n",
        "      paper_list=None\n",
        "    return paper_list\n",
        "\n",
        "\n",
        "def queryLearningPaperName(paperName):\n",
        "    try:\n",
        "      paper_list = query_papername(paperName, 5)\n",
        "    except:\n",
        "      paper_list=None\n",
        "    return paper_list\n",
        "\n",
        "\n",
        "def queryLearningPaperID(paperID):\n",
        "    try:\n",
        "      papers = query_paperid2(paperID)\n",
        "    except:\n",
        "      papers=None\n",
        "    return papers\n",
        "\n",
        "\n",
        "def queryField(field):\n",
        "    try:\n",
        "      paper_list = query_field(field, 10)\n",
        "    except:\n",
        "      paper_list=None\n",
        "    return paper_list\n",
        "\n",
        "\n",
        "# bot = telepot.Bot('5268776301:AAFMsXsNnIEBEsjVFXgLBJodhi65n_3L2VU')\n",
        "bot = telepot.Bot('5210472855:AAE2dw8gwD1ELvJCPylpDhws1cPLA6rY2lk')\n",
        "MessageLoop(bot, handle).run_as_thread()\n",
        "print(\"I'm listening...\")\n",
        "while 1:\n",
        "    time.sleep(5)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "iResearch_deployment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}